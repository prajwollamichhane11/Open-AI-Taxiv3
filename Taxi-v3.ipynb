{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/prajwol/anaconda3/lib/python3.6/site-packages (0.15.4)\r\n",
      "Requirement already satisfied: opencv-python in /home/prajwol/anaconda3/lib/python3.6/site-packages (from gym) (4.1.2.30)\r\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/prajwol/anaconda3/lib/python3.6/site-packages (from gym) (1.17.4)\r\n",
      "Requirement already satisfied: scipy in /home/prajwol/anaconda3/lib/python3.6/site-packages (from gym) (1.2.1)\r\n",
      "Requirement already satisfied: six in /home/prajwol/anaconda3/lib/python3.6/site-packages (from gym) (1.13.0)\r\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /home/prajwol/anaconda3/lib/python3.6/site-packages (from gym) (1.2.2)\r\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /home/prajwol/anaconda3/lib/python3.6/site-packages (from gym) (1.3.2)\r\n",
      "Requirement already satisfied: future in /home/prajwol/anaconda3/lib/python3.6/site-packages (from pyglet<=1.3.2,>=1.2.0->gym) (0.18.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yellow color = No Passenger\n",
    "# Green Color = Passenger\n",
    "# R,G,B,Y Passenger picking and dropping position\n",
    "# Blue Color = Pick Up Location\n",
    "# Purple Color = Drop off Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 actions (4 directions and 2 pick up and droping location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward +20 on exact location dropping off\n",
    "# -10 on illegal dropping and picking up\n",
    "# If taxi roams around unnecessary steps but stil finally drops to exact location, it is still penalized by -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States Depend Upon\n",
    "# Taxi location which can be in 5 X 5 position any where\n",
    "# Pick up and Drop off locations. The possible locations is 4\n",
    "# Passenger can be in any pick up and drop off location as well as on any illegal location i.e 5 locations\n",
    "# Hence, the no. of combinations we can represent the state is 5 X 5 X 4 X 5 \n",
    "# Hence, we obtain the observation space as Discrete(500) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "# env.render()\n",
    "# print(f\"Action Space: {env.action_space}\")\n",
    "# print(f\"Observation Space: {env.observation_space}\")\n",
    "q_table = np.zeros([env.observation_space.n,env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "\n",
    "# After 100001 iterations the taxi is likely to drop patient\n",
    "for i in range(1,100001):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done  = False\n",
    "    \n",
    "    while not done:\n",
    "        \n",
    "        action = np.argmax(q_table[state]) #Greedy\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        old_value = q_table[state,action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = old_value + alpha * (reward + gamma*next_max - old_value)\n",
    "        q_table[state,action] = new_value\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = np.argmax(q_table[state])\n",
    "    Next Class\n",
    "    state,reward,done,info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics Studies\n",
    "# 1. Reward, Action, Observation\n",
    "# 2. Model based VS Model Free ==> Reinforcement Learning\n",
    "# 3. Policy and Value Function\n",
    "# 4. Bellman Equation for Policy and Value Function\n",
    "# 5. Policy Iteration, Value Iteration and Q-Learning\n",
    "# 6. Markovian Decision Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
